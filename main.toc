\babel@toc {russian}{}\relax 
\contentsline {chapter}{\chaptername ~1.\hskip 1em Общие термины и обозначения}{4}{chapter.1}%
\contentsline {section}{\numberline {1.1.\hskip 1em}Модели зависимости и обучения}{4}{section.1.1}%
\contentsline {section}{\numberline {1.2.\hskip 1em}Функция потерь и функционал качества}{4}{section.1.2}%
\contentsline {section}{\numberline {1.3.\hskip 1em}Линейные модели классификации и регрессии}{10}{section.1.3}%
\contentsline {section}{\numberline {1.4.\hskip 1em}Метод наименьших квадратов}{10}{section.1.4}%
\contentsline {section}{\numberline {1.5.\hskip 1em}Скользящий контроль}{11}{section.1.5}%
\contentsline {section}{\numberline {1.6.\hskip 1em}Определения и обозначения}{12}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1.\hskip 1em}Процедура скользящего контроля}{12}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2.\hskip 1em}Доверительное оценивание}{13}{subsection.1.6.2}%
\contentsline {subsection}{\numberline {1.6.3.\hskip 1em}Стратификация}{14}{subsection.1.6.3}%
\contentsline {section}{\numberline {1.7.\hskip 1em}Разновидности скользящего контроля}{14}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1.\hskip 1em}Полный скользящий контроль (complete CV)}{14}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2.\hskip 1em}Случайные разбиения}{15}{subsection.1.7.2}%
\contentsline {subsection}{\numberline {1.7.3.\hskip 1em}Контроль на отложенных данных (hold-out CV)}{15}{subsection.1.7.3}%
\contentsline {subsection}{\numberline {1.7.4.\hskip 1em}Контроль по отдельным объектам (leave-one-out CV)}{16}{subsection.1.7.4}%
\contentsline {subsection}{\numberline {1.7.5.\hskip 1em}Контроль по q блокам (q-fold CV)}{16}{subsection.1.7.5}%
\contentsline {subsection}{\numberline {1.7.6.\hskip 1em}Контроль по r×q блокам (r×q-fold CV)}{16}{subsection.1.7.6}%
\contentsline {section}{\numberline {1.8.\hskip 1em}Скользящий контроль в задачах прогнозирования}{17}{section.1.8}%
\contentsline {subsection}{\numberline {1.8.1.\hskip 1em}Контроль с нарастающей длиной обучения}{17}{subsection.1.8.1}%
\contentsline {subsection}{\numberline {1.8.2.\hskip 1em}Контроль с фиксированной длиной обучения}{17}{subsection.1.8.2}%
\contentsline {section}{\numberline {1.9.\hskip 1em}Недостатки скользящего контроля}{18}{section.1.9}%
\contentsline {section}{\numberline {1.10.\hskip 1em}Применение скользящего контроля}{18}{section.1.10}%
\contentsline {chapter}{Список литературы}{20}{section.1.10}%
\contentsline {section}{\numberline {1.11.\hskip 1em}Вероятность переобучения}{29}{section.1.11}%
\contentsline {section}{\numberline {1.12.\hskip 1em}Статистические критерии в машинном обучении}{34}{section.1.12}%
\contentsline {subsection}{\numberline {1.12.1.\hskip 1em}Теория}{34}{subsection.1.12.1}%
\contentsline {subsection}{\numberline {1.12.2.\hskip 1em}Задачи}{36}{subsection.1.12.2}%
\contentsline {section}{\numberline {1.13.\hskip 1em}Кросс-валидация}{36}{section.1.13}%
\contentsline {subsection}{\numberline {1.13.1.\hskip 1em}Теория}{36}{subsection.1.13.1}%
\contentsline {subsubsection}{Hold-out}{37}{section*.57}%
\contentsline {subsubsection}{k-Fold}{38}{section*.59}%
\contentsline {subsection}{\numberline {1.13.2.\hskip 1em}Задачи (\href {https://education.yandex.ru/handbook/ml/article/kross-validaciya}{источник})}{39}{subsection.1.13.2}%
\contentsline {subsubsection}{Задача 1}{39}{section*.61}%
\contentsline {subsubsection}{Задача 2}{39}{section*.64}%
\contentsline {subsubsection}{Задача 3}{39}{section*.65}%
\contentsline {chapter}{\chaptername ~2.\hskip 1em Линейные методы классификации и регрессии}{43}{chapter.2}%
\contentsline {section}{\numberline {2.1.\hskip 1em}О регуляризации}{43}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1.\hskip 1em}Гауссовский и лапласовский регуляризаторы}{44}{subsection.2.1.1}%
\contentsline {subsubsection}{Лапласовский регуляризатор (L1 регуляризатор)}{44}{section*.68}%
\contentsline {subsubsection}{Гауссовский регуляризатор (L2 регуляризатор)}{44}{section*.69}%
\contentsline {subsubsection}{Сравнений L1 и L2 регуляризаторов}{45}{section*.70}%
\contentsline {subsection}{\numberline {2.1.2.\hskip 1em}Вероятностная интерпретация регуляризации}{45}{subsection.2.1.2}%
\contentsline {subsubsection}{L1 регуляризация}{46}{section*.73}%
\contentsline {subsubsection}{L2 регуляризация}{46}{section*.74}%
\contentsline {subsection}{\numberline {2.1.3.\hskip 1em}Задачи}{46}{subsection.2.1.3}%
\contentsline {subsubsection}{Вопрос 1}{46}{section*.75}%
\contentsline {subsubsection}{Вопрос 2}{47}{section*.76}%
\contentsline {subsubsection}{Вопрос 3}{47}{section*.78}%
\contentsline {section}{\numberline {2.2.\hskip 1em}Метод наименьших квадратов (МНК) в общем случае}{61}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1.\hskip 1em}Про линейную регрессию и МНК}{61}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2.\hskip 1em}МНК в общем случае}{61}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3.\hskip 1em}Проблемы и ограничения МНК}{61}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4.\hskip 1em}Задачи}{62}{subsection.2.2.4}%
\contentsline {section}{\numberline {2.3.\hskip 1em}Вероятностные функции потерь}{64}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1.\hskip 1em}Принцип максимума правдоподобия}{64}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2.\hskip 1em}Связь правдоподобия и аппроксимации эмпирического риска}{65}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3.\hskip 1em}Вероятностный смысл регуляризации}{65}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4.\hskip 1em}Задачи}{66}{subsection.2.3.4}%
\contentsline {subsubsection}{Задача 1.}{66}{section*.111}%
\contentsline {subsubsection}{Задача 2.}{66}{section*.112}%
\contentsline {subsubsection}{Задача 3.}{67}{section*.113}%
\contentsline {section}{\numberline {2.4.\hskip 1em}Методы оценки и проверки моделей}{67}{section.2.4}%
\contentsline {section}{\numberline {2.5.\hskip 1em}Линейная классификация}{70}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1.\hskip 1em}Постановка задачи}{70}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2.\hskip 1em}Многоклассовая классификация}{71}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3.\hskip 1em}Задачи}{71}{subsection.2.5.3}%
\contentsline {section}{\numberline {2.6.\hskip 1em}Линейные методы для интерпретации моделей}{72}{section.2.6}%
\contentsline {subsubsection}{Коэффициент детерминации \( R^2 \)}{74}{section*.129}%
\contentsline {section}{\numberline {2.7.\hskip 1em}Многоклассовая классификация}{79}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1.\hskip 1em}Один против всех (one-versus-all)}{79}{subsection.2.7.1}%
\contentsline {subsection}{\numberline {2.7.2.\hskip 1em}Все против всех (all-versus-all)}{82}{subsection.2.7.2}%
\contentsline {subsection}{\numberline {2.7.3.\hskip 1em}Многоклассовая логистическая регрессия}{83}{subsection.2.7.3}%
\contentsline {section}{\numberline {2.8.\hskip 1em}Логистиеская регрессия}{84}{section.2.8}%
\contentsline {chapter}{\chaptername ~3.\hskip 1em Основы нейросетевых моделей}{89}{chapter.3}%
\contentsline {section}{\numberline {3.1.\hskip 1em}Полносвязная нейронная сеть (Персептрон).}{89}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1.\hskip 1em}Модель нейрона МакКаллока-Питтса}{89}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2.\hskip 1em}Реализация логических функций с помощью нейрона}{89}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3.\hskip 1em}Область применимости многослойных нейронных сетей}{92}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4.\hskip 1em}Полносвязная нейронная сеть}{92}{subsection.3.1.4}%
\contentsline {subsection}{\numberline {3.1.5.\hskip 1em}Градиентный спуск}{93}{subsection.3.1.5}%
\contentsline {subsection}{\numberline {3.1.6.\hskip 1em}Стохастический градиентный спуск}{94}{subsection.3.1.6}%
\contentsline {subsection}{\numberline {3.1.7.\hskip 1em}Метод обратного распространения ошибок (BackProp).}{95}{subsection.3.1.7}%
\contentsline {subsection}{\numberline {3.1.8.\hskip 1em}Алгоритм применения метода обратного распространения ошибки в стохастическом градиентном спуске.}{99}{subsection.3.1.8}%
\contentsline {section}{\numberline {3.2.\hskip 1em}Функции активации ReLU и PReLU. Проблема «паралича» сети.}{100}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1.\hskip 1em}Про функции активации}{100}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2.\hskip 1em}ReLU (Rectified Linear Unit), проблема «паралича» сети}{100}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3.\hskip 1em}PReLU (Parametric Rectified Linear Unit)}{101}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4.\hskip 1em}Задачи}{101}{subsection.3.2.4}%
\contentsline {section}{\numberline {3.3.\hskip 1em}Drop Out}{103}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1.\hskip 1em}Теоретические сведения}{103}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2.\hskip 1em}Реализация}{103}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3.\hskip 1em}Задачи}{104}{subsection.3.3.3}%
\contentsline {section}{\numberline {3.4.\hskip 1em}CNN. Свёртки и пулинги для обработки изображений.}{105}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1.\hskip 1em}Стандартная схема свёрточной сети.}{105}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2.\hskip 1em}Приложения CNN.}{107}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3.\hskip 1em}Обобщение CNN на любые структурированные данные.}{108}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4.\hskip 1em}Задачи}{108}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5.\hskip 1em}Методы оптимизации с использованием Autograd: SGD, Adam, RMSProp}{114}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1.\hskip 1em}Автоматическое дифференцирование с Autograd}{114}{subsection.3.5.1}%
\contentsline {subsubsection}{Пример использования Autograd}{114}{section*.177}%
\contentsline {subsection}{\numberline {3.5.2.\hskip 1em}Стохастический градиентный спуск (SGD)}{115}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3.\hskip 1em}RMSProp}{115}{subsection.3.5.3}%
\contentsline {subsection}{\numberline {3.5.4.\hskip 1em}Adam}{116}{subsection.3.5.4}%
\contentsline {subsection}{\numberline {3.5.5.\hskip 1em}Задачи}{116}{subsection.3.5.5}%
\contentsline {chapter}{\chaptername ~4.\hskip 1em Метрические методы классификации и регрессии}{118}{chapter.4}%
\contentsline {section}{\numberline {4.1.\hskip 1em}Обобщенный метрический классификатор}{118}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1.\hskip 1em}Гипотеза непрерывности и компактности}{118}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2.\hskip 1em}Формализация расстояния}{118}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3.\hskip 1em}Метод k ближайших соседей (nearest neighbours, kNN)}{119}{subsection.4.1.3}%
\contentsline {subsection}{\numberline {4.1.4.\hskip 1em}Задача 1}{120}{subsection.4.1.4}%
\contentsline {subsection}{\numberline {4.1.5.\hskip 1em}Задача 2}{120}{subsection.4.1.5}%
\contentsline {subsection}{\numberline {4.1.6.\hskip 1em}Задача 3}{120}{subsection.4.1.6}%
\contentsline {section}{\numberline {4.2.\hskip 1em}Задачи}{125}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1.\hskip 1em}Задача 1}{125}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2.\hskip 1em}Ответ:}{125}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3.\hskip 1em}Задача 2}{126}{subsection.4.2.3}%
\contentsline {subsection}{\numberline {4.2.4.\hskip 1em}Ответ:}{126}{subsection.4.2.4}%
\contentsline {subsection}{\numberline {4.2.5.\hskip 1em}Задача 3}{126}{subsection.4.2.5}%
\contentsline {subsection}{\numberline {4.2.6.\hskip 1em}Ответ:}{126}{subsection.4.2.6}%
\contentsline {section}{\numberline {4.3.\hskip 1em}Формула Надарая-Ватсона.}{127}{section.4.3}%
\contentsline {section}{\numberline {4.4.\hskip 1em}Влияние выбора метрики на качество работы kNN}{130}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1.\hskip 1em}Сущность метода k-ближайших соседей}{130}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2.\hskip 1em}Популярные метрики расстояния}{131}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3.\hskip 1em}Влияние выбора метрики на качество работы модели}{132}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4.\hskip 1em}Примеры задач}{133}{subsection.4.4.4}%
\contentsline {subsection}{\numberline {4.4.5.\hskip 1em}Заключение}{133}{subsection.4.4.5}%
\contentsline {section}{\numberline {4.5.\hskip 1em}Более быстрые оптимизации kNN.}{134}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1.\hskip 1em}BallTree}{134}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2.\hskip 1em}KD-Tree}{135}{subsection.4.5.2}%
\contentsline {subsection}{\numberline {4.5.3.\hskip 1em}Примеры задач}{136}{subsection.4.5.3}%
\contentsline {section}{\numberline {4.6.\hskip 1em}Расстояние Махаланобиса в метрических методах классификации и регрессии}{137}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1.\hskip 1em}Определение:}{138}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2.\hskip 1em}Преимущества расстояния Махаланобиса:}{138}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3.\hskip 1em}Недостатки расстояния Махаланобиса:}{138}{subsection.4.6.3}%
\contentsline {subsection}{\numberline {4.6.4.\hskip 1em}Практические аспекты использования:}{139}{subsection.4.6.4}%
\contentsline {subsection}{\numberline {4.6.5.\hskip 1em}Применение в методах классификации и регрессии}{139}{subsection.4.6.5}%
\contentsline {subsection}{\numberline {4.6.6.\hskip 1em}Задачи}{140}{subsection.4.6.6}%
\contentsline {section}{\numberline {4.7.\hskip 1em}Профиль компактности и оценка обобщающей способности}{143}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1.\hskip 1em}CCV}{143}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2.\hskip 1em}Профиль компактности}{143}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3.\hskip 1em}CCV для метода 1NN}{143}{subsection.4.7.3}%
\contentsline {subsection}{\numberline {4.7.4.\hskip 1em}Пример профилей компактности}{144}{subsection.4.7.4}%
\contentsline {subsection}{\numberline {4.7.5.\hskip 1em}Задачи}{145}{subsection.4.7.5}%
\contentsline {subsection}{\numberline {4.7.6.\hskip 1em}Минимизация CСV}{146}{subsection.4.7.6}%
\contentsline {subsection}{\numberline {4.7.7.\hskip 1em}Random projection trees - \textbf {Annoy}}{146}{subsection.4.7.7}%
\contentsline {section}{\numberline {4.8.\hskip 1em}Отбор эталонных объектов}{150}{section.4.8}%
\contentsline {section}{\numberline {4.9.\hskip 1em}Компактность и профиль компактности}{151}{section.4.9}%
\contentsline {section}{\numberline {4.10.\hskip 1em}Метод окна Парзена.}{154}{section.4.10}%
\contentsline {section}{\numberline {4.11.\hskip 1em}Метод потенциальных функций}{158}{section.4.11}%
\contentsline {subsection}{\numberline {4.11.1.\hskip 1em}Подбор параметров}{159}{subsection.4.11.1}%
\contentsline {subsection}{\numberline {4.11.2.\hskip 1em}Преимущества и недостатки}{159}{subsection.4.11.2}%
\contentsline {subsection}{\numberline {4.11.3.\hskip 1em}Задачи для самопроверки}{160}{subsection.4.11.3}%
\contentsline {section}{\numberline {4.12.\hskip 1em}Полный скользящий контроль (ССV, Complete Cross-Validation)}{162}{section.4.12}%
\contentsline {subsection}{\numberline {4.12.1.\hskip 1em}Понятие профиля компактности}{163}{subsection.4.12.1}%
\contentsline {subsection}{\numberline {4.12.2.\hskip 1em}Свойства профиля компактности и CCV}{164}{subsection.4.12.2}%
\contentsline {subsection}{\numberline {4.12.3.\hskip 1em}Задача отбора эталонов $\Omega \subseteq X^L$ (prototype learning)}{164}{subsection.4.12.3}%
\contentsline {subsection}{\numberline {4.12.4.\hskip 1em}Жадный отбор эталонов $\Omega $ по критерию $CCV(\Omega ) \to \min $}{165}{subsection.4.12.4}%
\contentsline {subsection}{\numberline {4.12.5.\hskip 1em}Задачи}{165}{subsection.4.12.5}%
\contentsline {chapter}{\chaptername ~5.\hskip 1em Метод опорных векторов}{169}{chapter.5}%
\contentsline {section}{\numberline {5.1.\hskip 1em}SVM-классификация}{169}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1.\hskip 1em}Постановка задачи}{169}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2.\hskip 1em}Математическая формализация}{169}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3.\hskip 1em}Условия разделимости}{169}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4.\hskip 1em}Оптимизационная задача}{169}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5.\hskip 1em}Двойственная задача}{170}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6.\hskip 1em}Ядра}{170}{subsection.5.1.6}%
\contentsline {subsection}{\numberline {5.1.7.\hskip 1em}Дискриминантная функция в ядровом пространстве}{170}{subsection.5.1.7}%
\contentsline {subsection}{\numberline {5.1.8.\hskip 1em}Мягкие границы}{171}{subsection.5.1.8}%
\contentsline {subsection}{\numberline {5.1.9.\hskip 1em}Задача 1}{171}{subsection.5.1.9}%
\contentsline {subsection}{\numberline {5.1.10.\hskip 1em}Задача 2}{172}{subsection.5.1.10}%
\contentsline {subsection}{\numberline {5.1.11.\hskip 1em}Задача 3}{173}{subsection.5.1.11}%
\contentsline {section}{\numberline {5.2.\hskip 1em}SVM-регрессия}{174}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1.\hskip 1em}Постановка задачи}{174}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2.\hskip 1em}Прямая задача}{174}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3.\hskip 1em}Преобразование задачи}{175}{subsection.5.2.3}%
\contentsline {subsection}{\numberline {5.2.4.\hskip 1em}Метод Лагранжа}{175}{subsection.5.2.4}%
\contentsline {subsection}{\numberline {5.2.5.\hskip 1em}Условия оптимальности}{176}{subsection.5.2.5}%
\contentsline {subsection}{\numberline {5.2.6.\hskip 1em}Двойственная задача}{176}{subsection.5.2.6}%
\contentsline {subsection}{\numberline {5.2.7.\hskip 1em}Построение финальной модели}{176}{subsection.5.2.7}%
\contentsline {subsection}{\numberline {5.2.8.\hskip 1em}Выбор ядра}{177}{subsection.5.2.8}%
\contentsline {subsection}{\numberline {5.2.9.\hskip 1em}Задача 1}{178}{subsection.5.2.9}%
\contentsline {subsection}{\numberline {5.2.10.\hskip 1em}Задача 2}{179}{subsection.5.2.10}%
\contentsline {subsection}{\numberline {5.2.11.\hskip 1em}Задача 3}{179}{subsection.5.2.11}%
\contentsline {section}{1-norm SVM (LASSO SVM)}{180}{section*.212}%
\contentsline {section}{Сравнение L2 и L1 регуляризации}{181}{section*.216}%
\contentsline {section}{Doubly Regularized SVM (Elastic Net SVM)}{182}{section*.219}%
\contentsline {subsection}{Elastic Net Analysis}{183}{section*.222}%
\contentsline {section}{Support Features Machine (SFM)}{183}{section*.224}%
\contentsline {section}{Relevance Features Machine (RFM)}{184}{section*.226}%
\contentsline {section}{Задачи}{184}{section*.228}%
\contentsline {subsection}{Задача 1}{184}{section*.229}%
\contentsline {subsection}{Ответ:}{184}{section*.230}%
\contentsline {subsection}{Задача 2}{185}{section*.231}%
\contentsline {subsection}{Ответ:}{185}{section*.232}%
\contentsline {subsection}{Задача 3}{186}{section*.233}%
\contentsline {subsection}{Ответ:}{186}{section*.234}%
\contentsline {section}{Ядерные функции машины опорных векторов}{186}{section*.235}%
\contentsline {subsection}{Задача 1}{188}{section*.236}%
\contentsline {subsection}{Решение}{188}{section*.237}%
\contentsline {subsection}{Задача 2}{188}{section*.238}%
\contentsline {subsection}{Решение}{188}{section*.239}%
\contentsline {subsection}{Задача 3}{188}{section*.240}%
\contentsline {subsection}{Решение}{188}{section*.241}%
\contentsline {section}{Relevance Vector Machine(RVM)}{189}{section*.242}%
\contentsline {subsection}{Идея метода релевантных векторов}{189}{section*.243}%
\contentsline {subsection}{Задачи по RVM}{190}{section*.246}%
\contentsline {section}{Резюме по линейным классификаторам}{191}{section*.247}%
\contentsline {section}{Обобщения линейного SVM. Ядра и спрямляющие пространства. SVM как двухслойная нейронная сеть.}{192}{section*.248}%
\contentsline {subsection}{Задача 1}{193}{section*.250}%
\contentsline {subsection}{Решение}{193}{section*.251}%
\contentsline {subsection}{Задача 2}{193}{section*.252}%
\contentsline {subsection}{Решение}{193}{section*.253}%
\contentsline {subsection}{Задача 3}{193}{section*.254}%
\contentsline {subsection}{Решение}{194}{section*.255}%
\contentsline {chapter}{\chaptername ~6.\hskip 1em Метод главных компонент}{195}{chapter.6}%
\contentsline {section}{Введение в метод главных компонент(PCA)}{195}{section*.257}%
\contentsline {section}{Задачи на использование метода главных компонент}{196}{section*.258}%
\contentsline {subsection}{Задача 1: Вклад признаков в главные компоненты}{196}{section*.259}%
\contentsline {subsection}{Задача 2: Ошибка "реконструкции" PCA}{197}{section*.260}%
\contentsline {subsection}{Задача 3: Построить критерий D-оптимальности для выбора лучших k-компонент }{199}{section*.261}%
\contentsline {section}{Денойзинг данных с помощью метода главных компонент}{200}{section*.262}%
\contentsline {section}{Задачи про денойзинг данных}{202}{section*.263}%
