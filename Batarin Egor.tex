\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[T2A]{fontenc} % Кодировка для кириллицы
\usepackage[utf8]{inputenc} % Кодировка ввода
\usepackage[russian]{babel} % Локализация и переносы для русского языка
\usepackage{amsmath}

\title{Автокодировщик, имитирующий PCA}
\author{Егор Батарин}
\date{December 2024}

\begin{document}

\maketitle

\section{Метод главных компонент}


Основной задачей метода главных компонент (он же PCA от Principal Component Analysis)  является минимизация ошибки восстановления данных, если они проецируются на подпространство меньшей размерности $k<d$, где $d$ - исходная размерность, а $k$ - уменьшенная. Функция потерь в этом случае задаётся как:

\[
L = \| \mathbf{X} - \mathbf{X}_{\text{proj}} \|_F^2,
\]

где:

\begin{itemize}
    \item \( \mathbf{X} \in \mathbb{R}^{n \times d} \) — исходная центрированная матрица данных, состоящая из $n$ векторов-строк из пространства размерности $d$;
    \item \( \mathbf{X}_{\text{proj}} = \mathbf{X} \mathbf{W} \mathbf{W}^\top \) — проекция данных на подпространство, заданное матрицей \( \mathbf{W} \);
    \item \( \| \cdot \|_F^2 \) — фробениусова норма, измеряющая суммарное отклонение всех точек данных от их проекций.
\end{itemize}

PCA минимизирует \( L \) путём выбора такого \( k \)-мерного подпространства, которое сохраняет максимальную часть дисперсии данных. \\

\textbf{Задача 1}. Докажите, что алгоритм последовательного построения главных компонент обладает свойством жадного отбора: локальные минимизации функции потерь путем добавления следующих главных компонент ведут к глобальной минимизации функции потерь для заданного числа главных компонент. 

\textit{Решение}
Действительно, алгоритм PCA заключается в следующем:\\

0. Если число главных компонент равно нулю, так, что мы проецируем данные на нульмерное пространство-точку, то оптимальной точкой будет начало координат, минимизирующее фробениусову норму

1. Выбираем первую главную компоненту (первый вектор базиса) так, чтобы выборочная дисперсия вдоль данной компоненты была минимальна. 

2. Вторую главную компоненту выбираем так, чтобы дисперсия данных вдоль нее была максимальна при условии ортогональности первой главной компоненте.

3. Аналогично выбираем третью главную компонету при условии ортогональности первой и второй. Далее действуем аналогично.


На каждом шаге будет получаться локально оптимальный выбор базиса главных компонент, при проекцировании данных на который функция потерь будет минимальна. Но поскольку каждый такой локальный базис является подбазисом объемлющего базиса и бОльшего числа главных компонент, который является глобально оптимальным, мы получаем, что локальные оптимизации функции потерь путем последовательного построения главных компонент приводят к глобально оптимальному базису.\\

\textbf{Задача 2}. Покажите, что $k$-ая главная компонента "важнее"  $k+1$ главной компоненты с точки зрения минимизации функции потерь.\\

\textit{Решение}. Это следует из вида функции потерь для $k$ главных компонент:

\[
\sum_{i=k+1}^d \lambda_i,
\]
где $\lambda_i$ - собственные числа ковариационной матрицы $\mathbf{\Sigma} = \frac{1}{n} \mathbf{X}^\top \mathbf{X}$, а также свойства собственных чисел: $ \lambda_{i+1} \leq \lambda_i$

\section{Автокодировщики в сравнении с PCA}

Автокодировщики представляют собой нейронные сети, предназначенные для обучения компактных представлений данных. Суть автокодировщика заключается в том, что он обучается на реконструкции исходных данных с помощью более компактной внутренней репрезентации (латентное пространство). В отличие от PCA, автокодировщики могут обучаться не только на линейных, но и на нелинейных зависимостях между признаками, что делает их более гибкими и мощными инструментами для снижения размерности.

Автокодировщик состоит из двух частей: энкодера и декодера. Энкодер преобразует исходные данные в скрытое представление (латентное пространство), а декодер восстанавливает исходные данные из этого представления. Обучение автокодировщика заключается в минимизации ошибки восстановления между исходными данными и их реконструкцией.

На лицо видна аналогия между PCA и автокодировщиками: 

1. Оба метода сжимают исходные данные: PCA в матрицу признаков меньшей размерности, а автокодировщик - в латентное пространство

2. Оба метода минимизируют фробениусову норму входной и выходной матриц: у PCA это матрица признаков, а у автокодировщиков, как правило, эта матрица (или тензор) представляет из себя изображение.

Однако у PCA есть полезные свойства, которыми не обладает обыкновенный автокодировщик: 

1. Главные компоненты отсортированы по обыванию их "важности" по минимизации функции потерь. Это позволяет жадным образом отбирать самые необходимые главные компоненты для сжатия данных.

2. Главные компоненты ортогональны и статистически независимы. Это повышает интерпретабельность PCA.



\section{Алгоритм PCAАE}

Оказывается можно построить автокодировщик, который обладает приведенными выше свойствами. Следуя авторам, будем называть его PCAAE (Principal Component Analysis Autoencoder) или PCA-автокодировщик.

Перед тем как описать PCA-автокодировщик, сначала определим некоторые обозначения. Пусть \( \mathbf{X} \) — это пространство данных, в общем случае мы будем рассматривать изображения размера \( n \times n \), то есть \( \mathbf{X} = \mathbb{R}^{n \times n} \). Обозначим латентное пространство как \( \mathbf{Z} = \mathbb{R}^d \), где \( d \) — размерность этого латентного пространства. Обозначим кодировщик как \( E: \mathbf{X} \to \mathbf{Z} \), а декодировщик как \( D: \mathbf{X} \to \mathbf{Z} \). Обозначим \( z_i \) как \( i \)-й компонент вектора \( \mathbf{z} \). Пусть \( y = D \circ E(\mathbf{x}) \) — это вывод автокодировщика. Кроме того, обозначим \( \mathbf{x}^{(i)} \) как \( i \)-й образец данных, а его код \( \mathbf{z}^{(i)} = E(\mathbf{x}^{(i)}) \). Наконец, будем обозначать \( i \)-ю версию кодировщика как \( E^{(i)}: \mathbf{X} \to \mathbb{R}^i \), и аналогично для декодировщика.

Теперь опишем основную идею и алгоритм PCA-автокодировщика. Как мы уже объяснили, есть два центральных вопроса, которые мы должны решить для определения PCA-автокодировщика:
\begin{itemize}
    \item Что мы понимаем под «увеличением важности» компонент латентного пространства, и как мы можем это наложить?
    \item Как мы можем обеспечить независимость латентных кодов?
\end{itemize}

В случае PCA важность определяется вариативностью данных вдоль оси, однако такое определение трудно применить к автокодировщику, так как в процессе обучения, как правило, все размерности латентного пространства заполняются. Таким образом, просто выполнить PCA на латентном пространстве не получится.

Поэтому мы вводим понятие важности, обучая серию автокодировщиков с увеличивающимся размером латентного пространства, начиная с латентного пространства размерности 1 (скаляр). В этом первом автокодировщике мы можем предположить, что информация наибольшей «важности» будет закодирована, в смысле минимизации потерь \( \ell_2 \). Например, это может быть средний цвет фона. Затем мы увеличиваем размер латентного пространства на 1, сохраняя ту же первую компоненту из предыдущего обучения: обучается только вторая компонента. Этот процесс повторяется итеративно, пока не будет достигнут предустановленный размер \( d_{\max} \). Заметим, что на каждой итерации предыдущий декодировщик отбрасывается, и новый обучается с нуля. Действительно, мы хотим наложить структуру на латентное пространство через обучение кодировщика, но декодировщик должен иметь возможность делать это по своему усмотрению.

Мы решаем второй вопрос, как наложить независимость на латентные коды, следующим образом. Для этого потребуем, чтобы величина ковариации каждой компоненты латентного пространства была как можно меньше. Мы можем достичь этого, добавив дополнительный член в функцию потерь автокодировщика, который будет учитывать эту зависимость. Предположим, что мы добавляем компоненту \( k \) в латентное пространство. Тогда для пакета данных размером \( M \), \( X = \{ \mathbf{x}^{(1)}, \dots, \mathbf{x}^{(M)} \} \), мы можем вычислить ковариацию по формуле:
\[
L_{\text{cov}}(X) = \sum_{i=1}^{k-1} \left[ \frac{1}{M} \sum_{j=1}^{M} \left( z_i^{(j)} z_k^{(j)} - \frac{1}{M} \sum_{j=1}^{M} z_i^{(j)} \sum_{j=1}^{M} z_k^{(j)} \right) \right].
\]

Чтобы упросить задачу, мы до латентного слоя применим batch normalisation, тогда получим: 

$$L_{\text{cov}}(X) = \frac{1}{M} \sum_{i=1}^{k-1} \sum_{j=1}^{M} z_i^{(j)} z_k^{(j)}
$$\\

\textbf{Задача 3}. Докажите утверждение выше.\\

\textit{Решение}.
Формула для batch normalisation имеет вид:
\[
y_i = \gamma \left( \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \right)
\]
Таким образом получается центрированный набор данных с нулевым средним, поэтому  $\sum_{j=1}^{M} z_i^{(j)} = 0$.\\

Итоговая функция потерь будет иметь вид:

$$L(X) = \frac{1}{M} \sum_{i=1}^{M} \left\| x^{(i)} - D \circ E(x^{(i)}) \right\|_2^2 + \lambda L_{\text{cov}}(X)
$$

Минимизируя эту функцию, мы, во-первых, добиваемся того, чтобы выходное изображение было похоже на входное, во-вторых, добиваемся как можно меньшей статистической зависимости между компонентами латентного пространства. 

\begin{thebibliography}{99}


\bibitem{article1} Saïd Ladjal, Alasdair Newson \textit{A PCA-Like Autoencoder}, arviv.org  2 Apr 2019.

\bibitem{article2} Saïd Ladjal, Alasdair Newson \textit{PCAAE: Principal Component Analysis Autoencoder for organising the latent space of generative networks}, arviv.org 14 Jun 2020.

\end{thebibliography}


\end{document}
