\section{Введение в метод главных компонент(PCA)}
\textbf{Метод главных компонент (Principal Component Analysis, PCA)} — это статистический метод, используемый для снижения размерности данных с сохранением наиболее значимой информации. PCA находит новые признаки (главные компоненты), 
которые представляют собой линейные комбинации исходных признаков, причем эти компоненты ортогональны и ранжированы по величине объясняемой дисперсии.
\textbf{Основные этапы метода:} \\
1. \textbf{Центрирование данных}:\\ Данные центрируются так, чтобы среднее значение каждой переменной было равно нулю:
$$
X_c=X-\bar{X},
$$
где $X$ - исходная матрица данных (размер $n \times p$ ), $\bar{X}$ - вектор средних значений по столбцам.\\
2. \textbf{Построение ковариационной матрицы}: \\ Вычисляется ковариационная матрица:
$$
\Sigma=\frac{1}{n-1} X_c^T X_c
$$
где $\Sigma$ - симметричная матрица размером $p \times p$.\\
3. \textbf{Собственные значения и собственные векторы}: \\ Решается задача нахождения собственных значений и собственных векторов ковариационной матрицы:
$$
\Sigma \mathbf{v}_i=\lambda_i \mathbf{v}_i
$$
где $\lambda_i$ - собственные значения, $\mathbf{v}_i$ - соответствующие им собственные векторы.\\
4. \textbf{Ранжирование главных компонент}: \\ Собственные значения упорядочиваются по убыванию:
$$
\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p
$$
Первые несколько компонент, соответствующие самым большим собственным значениям, объясняют большую часть дисперсии данных.\\
5. \textbf{Проекция данных}: \\ Данные проецируются на главные компоненты:
$$
Z=X_c V_k,
$$
где $V_k$ — матрица $k$ собственных векторов, соответствующих $k$ наибольшим собственным значениям, $Z$ — матрица данных в пространстве главных компонент.

Свойства метода: \\
- Главные компоненты ортогональны:

$$
\mathbf{v}_i^T \mathbf{v}_j=0, \quad i \neq j
$$

- Дисперсия объясняется последовательностью собственных значений:

$$
\text { Объясненная дисперсия }=\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i} \text {. }
$$

\section{Спектральный метод наименьших квадратов}

Ridge регрессия отделяет наименьшие собственные значения матрицы от нуля путем добаваления $\tau$ ко всем значениям. Аналогично PCA отделяет наименьшие собственные значения от 0, но другим методом, а именно просто не учитывает их(отбрасывает). Как будто в суммах вместо них просто стоят 0. Обобщение данного подхода и называется \textbf{Спектральным методом наименьших квадратов: } \\
Приведем алгоритм: 
\begin{enumerate}
    \item Строим SVD (Singular Value Decomposition) разложение, и упорядочиваем собственные значения по возрастанию: $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n$
    \item Есил среди них есть близкие к нулю значения(то есть у нас будет плохая обусловленность нашей матрицы $\Rightarrow$ мультиколлинеарность при обучении), то нам нужно найти способ отделения от 0 этих собственных значений: $\lambda_j \rightarrow \lambda^{'}_j \ \forall j = \overline{m,n}$. При этом собственные векторы не меняем. \\
    Рассмотим частные случаи:
    \begin{itemize}
        \item $\lambda^{'}_j = \lambda_j + \tau$ - гребневая регрессия (ridge regression)
        \item $\lambda^{'}_j = \lambda_j + I_{[j > m]}\infty$ - метод главных компонент (PCA)      
    \end{itemize}

    \item Применим формулы SVD для модификации МНК-решения: \\
     $$\alpha^{*} = \sum_{j =1}^{n} \frac{1}{\sqrt{\lambda_j}} u_j(v_j^Ty)  \rightarrow \alpha^{*} = \sum_{j =1}^{n} \frac{\sqrt{\lambda_j}}{\lambda^{'}_j} u_j(v_j^Ty)$$
     
     $$F\alpha^{*} = \sum_{j =1}^{n} v_j(v_j^Ty) \rightarrow F\alpha^{*} = \sum_{j =1}^{n} \frac{\lambda_j}{\lambda^{'}_j} v_j(v_j^Ty)
     $$
\end{enumerate}
Интуиция данного метода заключается в том, что мы вводим поправки для близких к нулю собсвенных значений ($\delta_j$) уменьшая вклад этих самых собственных значений. Эти паправки мы вольны варировать практически как угодно от небольших сдвигов (Ridge regression) и вплоть до $\infty$ (PCA).

\section{Задача низкорангового матричного разложения}
Сам метод PCA позволяет:
\begin{itemize}
    \item понижать размерность в задачах регресси/классификации
    \item генерировать новые признаки 
    \item формировать сжатое представление данных
\end{itemize}
Но это все задачи \textbf{низкорангового матричного представления}. \\
В общем случая задача следующая: \\
\textbf{Дано:} Матрица $Z = ||z_{ij}||_{n*m}, (i, j) \in \Omega \subset \{1, \dots, n\}*\{1, \dots, m\}$  \\ 
\textbf{Найти:} матрицы $X=||x_{it}||_{n*k}$ и $Y = ||y_{tj}||_{k*m}$ такие, что:  
$$
||Z-XY||^{2} = \sum_{(i,j) \in \Omega} (z_{ij} - \sum_t x_{it}*y_{tj})^2 \rightarrow \min_{X, Y}
$$
Из за дополнительных ограничений, мы вынуждены откзаться от SVD:
\begin{itemize}
    \item не квадратичная функция потерь
    \item неотрицательное матричное разложение $x_{it} \geq 0$, $y_{tj} \geq 0$
    \item разряженные данные: $|\Omega| \ll nm$
\end{itemize}

\section{Задачи на использование метода главных компонент}

\subsection{Задача 1: Вклад признаков в главные компоненты}
Пусть $X \in R_{n\times p}$ набор данных с n образцами (строками) и p признаками (столбцами). PCA стремится найти набор собственных векторов (главных компонент), которые максимизируют дисперсию данных при проецировании на эти векторы.

Задача состоит в том, чтобы математически оценить, какой вклад вносит каждый признак в главные компоненты, и проранжировать признаки в зависимости от их вклада.\\ \\
\textbf{Решение:}
Метод РСА ищет собственные векторы $\mathbf{v}_i$ и собственные значения $\lambda_i$ удовлетворяющие:

$$
\Sigma \mathbf{v}_i=\lambda_i \mathbf{v}_i,
$$


где $\lambda_i$ - величина дисперсии данных вдоль $\mathbf{v}_i$.
Собственные векторы $\mathbf{v}_i$ формируют матрицу $V=\left[\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p\right]$, где каждый столбец $\mathbf{v}_i$ указывает направления главных компонент.\\
\textbf{Вклад признака в главные компоненты:}\\

Каждый признак в $X$ вносит вклад в главные компоненты через веса собственных векторов $\mathbf{v}_i$ . Элементы $v_{i j}$ (где $v_{i j}-j$-й элемент $i$-го собственного вектора) определяют значимость $j$-го признака для $i$-й главной компоненты.

Вклад $j$-го признака в $i$-ю главную компоненту оценивается как квадрат соответствующего элемента $v_{i j}^2$ .\\

Общий вклад $j$-го признака во все главные компоненты можно найти, суммируя его взвешенные вклады с учётом дисперсий ( $\lambda_i$ ):\\
$j=\sum_{i=1}^p \lambda_i v_{i j}^2$.\\
Этот показатель учитывает как значимость признака для каждой компоненты $\left(v_{i j}^2\right)$, так и долю дисперсии, объясняемую компонентой ( $\lambda_i$ ).\\
На конкретном примере:
Пусть собственные вектора образуют матрицу V:

$$
V=\left[\begin{array}{cccc}
0.5 & 0.6 & 0.3 & 0.1 \\
0.4 & -0.7 & 0.2 & 0.5 \\
-0.6 & 0.2 & 0.7 & -0.4 \\
0.5 & 0.3 & -0.6 & -0.6
\end{array}\right]
$$
Собственные значения:

$$
\Lambda=\operatorname{diag}(4.0,2.5,1.2,0.3)
$$

Посчитаем вклад признака $1(j=1)$ :
Для этого берём первую строчку $V$ :

$$
v_{1, \cdot}=[0.5,0.6,0.3,0.1]
$$
Считаем:

$$
\begin{aligned}
\text { Contribution }_1 & =\left(0.5^2 \cdot 4.0\right)+\left(0.6^2 \cdot 2.5\right)+\left(0.3^2 \cdot 1.2\right)+\left(0.1^2 \cdot 0.3\right) \\
& =1.0+0.9+0.108+0.003=2.011
\end{aligned}
$$

То же самое повторяем для остальных строк и находим максимальное значение.
\subsection{Задача 2: Ошибка "реконструкции" PCA}
Пусть $X \in R_{n\times p}$ набор данных с $n$ образцами (строками) и $p$ признаками (столбцами), с помощью метода главных компонент нужно:
\begin{itemize}
\item{Спроецируйте данные в более низкоразмерное пространство, определяемое $k$ главными компонентами.}
\item {Реконструируйте исходные данные из пространства пониженной размерности.}
\item {Вычислите ошибку реконструкции и оцените, как она меняется в зависимости от количества сохраняемых компонент k.}
\end{itemize}
\textbf{Решение:}
Для реконструкции данных из $k$-мерного подпространства используется обратная проекция:

$$
\hat{X}=Z V_k^T+\bar{X}
$$


Здесь:
- $Z V_k^T$ возвращает проекцию данных в исходное $p$-мерное пространство.\\
- Добавление $\bar{X}$ восстанавливает исходное смещение данных. \\
Ошибка реконструкции должна показывать какую часть информации мы потеряли при использовании только $k$ компонент при репрезентации данных.\\
1. Определим ошибку реконструкции для одного объекта $x_i$ :

$$
E_i=\left\|x_i-\hat{x}_i\right\|^2=\left\|\left(x_i-\bar{X}\right)-\left(z_i V_k^{\top}\right)\right\|^2
$$

2. Обобщим на весь набор данных:

$$
E=\frac{1}{n \times p} \sum_{i=1}^n\left\|x_i-\hat{x}_i\right\|^2
$$

3. Заменим на выражение для $\hat{x}_i$:

$$
E=\frac{1}{n \times p} \sum_{i=1}^n\left\|x_i-\bar{X}-Z V_k^{\top}\right\|^2
$$
Мы получили выражение для ошибки реконструкции. Теперь докажем, что 
ошибка реконструкции $E$ уменьшается монотонно с $k$, и когда $k=p, E=0$.

1. Общая дисперсия данных - это след ковариационной матрицы, которая представляет собой сумму всех собственных значений:

$$
\text { Total Variance }=\sum_{j=1}^p \lambda_j
$$

2. Дисперсия, которую уловили $k$ компонент это:

$$
\text { Captured Variance }=\sum_{j=1}^k \lambda_j
$$

3. Ошибка реконструкции по сути является дисперсией, которую не удалось уловить, то есть просто:

$$
E=\text { Total Variance }- \text { Captured Variance }=\sum_{j=k+1}^p \lambda_j
$$

4. Так как $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0$, добавление большего числа компонент ( $k \rightarrow k+1$ ) уменьшает $E$ :

$$
\sum_{j=k+1}^p \lambda_j>\sum_{j=k+2}^p \lambda_j
$$

5. В тот момент, когда $k=p, \sum_{j=k+1}^p \lambda_j=0$ $\Rightarrow$ $E=0$.

\subsection{Задача 3: Построить критерий D-оптимальности для выбора лучшиз k-компонент }
Набор данных представляет собой матрицу $n \times p$, где n - число образцов (строк), а p - число признаков (столбцов). Введём критерий D-оптимальности, используемый для выбора подмножества точек из набора данных, которое максимизирует детерминант информационной матрицы.
$$
D_{opt} : max  det(X^{T}X)
$$

Ключевым свойством критерия D-оптимальности является то, что он максимизирует объём многомерной фигуры, которая получается из рассматриваемых признаков. \\
Нужно построить критерий D-оптимальности для выбора лучших 
k главных компонент, которые максимизируют детерминант объясненной дисперсии (или объём фигуры) в k-мерном подпространстве PCA. 

\textbf{Решение:}

Критерий D-оптимальности для подпространства $k$ задаётся максимизацией детерминанта информационной матрицы $\Lambda_k$ :

$$
D_{o p t}(k)=\max \operatorname{det}\left(\Lambda_k\right)
$$

Так как $\Lambda_k$ является диагональной матрицей, её детерминант равен произведению собственных значений:

$$
\operatorname{det}\left(\Lambda_k\right)=\prod_{i=1}^k \lambda_i
$$

Итак, наша задача сводится к выбору $k$-мерного подпространства (т.е. первых $k$ главных компонент), которые максимизируют произведение $\lambda_1 \cdot \lambda_2 \cdots \cdot \lambda_k$, что эквивалентно решению следующей задачи:

$$
\max _{V_k} \prod_{i=1}^k \lambda_i
$$


где $\lambda_i$ - собственные значения матрицы ковариации $\Sigma$.
Для вычисления $D_{\text {opt }}(k)$ :\\
\\
1. Центрируем данные:

$$
X_c=X-\bar{X},
$$


где $\bar{X}$ - матрица средних значений.\\
2. Вычисляем ковариационную матрицу:

$$
\Sigma=\frac{1}{n-1} X_c^T X_c
$$

3. Находим собственные значения $\lambda_1, \lambda_2, \ldots, \lambda_p$ и соответствующие собственные векторы $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p$.\\
4. Выбираем первые $k$ собственных значений $\lambda_1, \lambda_2, \ldots, \lambda_k$, которые максимизируют:

$$
\prod_{i=1}^k \lambda_i
$$


Максимизация $\prod_{i=1}^k \lambda_i$ эквивалентна максимизации объёма \textbf{$k$-мерного эллипсоида}, описывающего данные в пространстве первых $k$ главных компонент. Это позволяет отобрать $k$ измерений, которые сохраняют максимальную дисперсию данных.

\subsection{Аппроксимация полиномиальной функции}
Дана функция $f(x) = e^x + x^3$ на интервале $x \in [0, 1]$. Постройте аппроксимацию функции $f(x)$ с помощью полинома $P(x) = a_0 + a_1 x + a_2 x^2$, используя СМНК и ортогональную систему функций $\{1, x, x^2\}$.

\subsubsection*{Решение:}
1. \textbf{Определяем матрицу системы}:
\begin{align*}
A_{ij} &= \int_0^1 \phi_i(x) \phi_j(x) dx, \\
\vec{b}_i &= \int_0^1 f(x) \phi_i(x) dx,
\end{align*}
где $\phi_i(x) = x^{i-1}$ для $i = 1, 2, 3$.
2. \textbf{Вычисляем элементы матрицы $A$ и вектора $\vec{b}$}:
\[
A = \begin{bmatrix}
\int_0^1 1 dx & \int_0^1 x dx & \int_0^1 x^2 dx \\
\int_0^1 x dx & \int_0^1 x^2 dx & \int_0^1 x^3 dx \\
\int_0^1 x^2 dx & \int_0^1 x^3 dx & \int_0^1 x^4 dx \\
\end{bmatrix}.
\]
После интегрирования:
\[
A = \begin{bmatrix}
1 & \frac{1}{2} & \frac{1}{3} \\
\frac{1}{2} & \frac{1}{3} & \frac{1}{4} \\
\frac{1}{3} & \frac{1}{4} & \frac{1}{5} \\
\end{bmatrix}.
\]
Для $\vec{b}$:
\[
b_i = \int_0^1 f(x) x^{i-1} dx.
\]
Вычисляем $\vec{b}$ численно или аналитически:
\[
b_1 = \int_0^1 (e^x + x^3) dx, \quad b_2 = \int_0^1 (e^x + x^3)x dx, \quad b_3 = \int_0^1 (e^x + x^3)x^2 dx.
\]

3. \textbf{Решаем систему уравнений} $A \vec{a} = \vec{b}$ для $\vec{a} = [a_0, a_1, a_2]^T$.

\subsection{Спектральная аппроксимация периодического сигнала}
Дан периодический сигнал $f(x) = \sin(2\pi x) + \cos(4\pi x)$ на $x \in [0, 1]$. Аппроксимируйте его с помощью линейной комбинации ортогональных функций $\{\sin(2\pi x), \cos(4\pi x), \sin(6\pi x)\}$, используя СМНК.

\subsubsection*{Решение:}
1. \textbf{Функции аппроксимации}:
\[
P(x) = a_1 \sin(2\pi x) + a_2 \cos(4\pi x) + a_3 \sin(6\pi x).
\]

2. \textbf{Матрица системы}:
Поскольку $\{\sin, \cos\}$ ортогональны на $[0, 1]$, матрица $A$ будет диагональной:
\[
A_{ij} = \int_0^1 \phi_i(x) \phi_j(x) dx.
\]
Элементы матрицы:
\[
A = \begin{bmatrix}
\frac{1}{2} & 0 & 0 \\
0 & \frac{1}{2} & 0 \\
0 & 0 & \frac{1}{2} \\
\end{bmatrix}.
\]

3. \textbf{Вектор правой части}:
\[
b_i = \int_0^1 f(x) \phi_i(x) dx.
\]
Для $\phi_1(x) = \sin(2\pi x)$, $\phi_2(x) = \cos(4\pi x)$, $\phi_3(x) = \sin(6\pi x)$:
\[
b_1 = \frac{1}{2}, \quad b_2 = \frac{1}{2}, \quad b_3 = 0.
\]

4. \textbf{Решение}:
\[
a_1 = 1, \quad a_2 = 1, \quad a_3 = 0.
\]

\subsection{Аппроксимация данных измерений}
Даны экспериментальные данные:
\[
x = [0, 1, 2, 3, 4], \quad y = [1, 2.1, 3.8, 6.3, 10.1].
\]
Аппроксимируйте их квадратичной функцией $P(x) = a_0 + a_1 x + a_2 x^2$ с помощью СМНК.

\subsubsection*{Решение:}
1. \textbf{Система нормальных уравнений}:
\[
A = \begin{bmatrix}
5 & \sum x_i & \sum x_i^2 \\
\sum x_i & \sum x_i^2 & \sum x_i^3 \\
\sum x_i^2 & \sum x_i^3 & \sum x_i^4 \\
\end{bmatrix}, \quad 
\vec{b} = \begin{bmatrix}
\sum y_i \\
\sum x_i y_i \\
\sum x_i^2 y_i \\
\end{bmatrix}.
\]

2. \textbf{Вычисления}:
\[
A = \begin{bmatrix}
5 & 10 & 30 \\
10 & 30 & 100 \\
30 & 100 & 354 \\
\end{bmatrix}, \quad 
\vec{b} = \begin{bmatrix}
23.3 \\
79.2 \\
278.4 \\
\end{bmatrix}.
\]

3. \textbf{Решение системы} $A \vec{a} = \vec{b}$:
Используем метод Гаусса или численные методы для нахождения $\vec{a}$. Получаем:
\[
a_0 = 0.9, \quad a_1 = 1.2, \quad a_2 = 0.3.
\]
Функция аппроксимации:
\[
P(x) = 0.9 + 1.2x + 0.3x^2.
\]
