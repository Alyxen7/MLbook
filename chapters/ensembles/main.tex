\section{Композиции классификаторов}

В задачах классификации, регрессии и прогнозирования нередки ситуации,    когда ни один алгоритм не обеспечивает нужного качества исходной модели.  В таких случаях на помощь приходит метод композиции классификаторов, также известный как ансамблевые методы, который сочетает несколько моделей для улучшения точности и устойчивости предсказаний по сравнению с использованием одиночного классификатора. Идея применения метода зключается в том, что объединение различных алгоритмов, каждый из которых может иметь свои сильные и слабые стороны, позволяет компенсировать недостатки отдельных моделей и достигать лучших результатов.

\subsection{Основные понятия}

\begin{itemize}
    \item $X^{l \cdot n} = (x_1, ... , x_\textit{l})$ --- обучающая выборка; $Y^\textit{l} = (y_1, ... , y_\textit{l})$ - вектор ответов;
\item $a_t: X \to Y, \ t = 1, \dots, T$ --- обучаемые алгоритмы $a_t: X \to Y$, аппроксимирующие неизвестную целевую зависимость $y_i = y^*(x_i)$.
\end{itemize}
   \textbf{Идея ансамблирования} (И.Ю.Журавлев): как из множества по отдельности плохих алгоритмов \textit{$ a_t(x) $} сделать один хороший?\\ 
   \\ 
   \textbf{Понятие композиции базовых алгоритмов}  
   
   Любой базовый алгоритм представим в следующем виде: $ a_t(x) = C(b_t(x))$ 
   
   $a_t(x): \textit{X}  \overset{b_t}{\longrightarrow} \textit{R} \overset{C}{\longrightarrow} \textit{Y}$, где \textit{C} --- решающее правило, \textit{R} --- удобное пространство оценок, $b_t$ - алгоритмические операторы.

    Основная идея ансамблирования заключается в декомпозиции базового алгоритма: создании \textbf{ансамбля} \textit{агрегирующей операции F }, которая комбинирует результаты нескольких базовых алгоритмов для повышения точности предсказаний перед использованием решающего правила: 

\ \
    
    $F: \mathbb{R}^T \to \mathbb{R}$ – агрегирующая операция базовых алгоритмов $a_1, \dots, a_T$

    $$a(x) = C(F(b_1(x), \dots, b_T(x)))$$

    
Суперпозиции вида $F(b_1, ... , b_T )$ являются отображениями из X в $\mathbb{R}$, то есть, опять-таки, алгоритмическими операторами. Это позволяет строить иерархические композиции, применяя определение ансамбля рекурсивно.
Ранее не существовало четкого разделения алгоритма на две составляющие. Выбор функции ансамблирования позволяет  агрегировать результат нескольких моделей для повышения итоговой точности предсказаний.

\ \
\begin{itemize}
    \item \textbf{Пример 1:} Классификация, $Y$ – конечное множество.
    
    В этом случае $R = Y$, а $C(b) \equiv b$ – решающее правило не используется.
    \item \textbf{Пример 2:} Классификация на 2 класса, $Y = \{-1, +1\}$, алгоритм имеет вид:
    $$a(x) = \text{sign}(b(x)),$$
    В этом случае алгоритмические операторы называют также вещественнозначными классификаторами \textit{(real-valued classifiers)}: $R = \mathbb{R}$, $b: X \to \mathbb{R}$, $C(b) \equiv \text{sign}(b)$
\end{itemize}

\subsection{Агрегирующие функции}
Агрегирующая функция должна удовлетворять ряду требованиям для обеспечения эффективного комбинирования предсказаний отдельных моделей. Перечислим их:
\begin{itemize}
    \item $F(b_1, \dots, b_T) \in [\min_t b_t, \max_t b_t]$ – среднее по Коши $\forall x$;
    \item $F(b_1, \dots, b_T)$ монотонно не убывает по всем $b_t$;
    \item \textbf{Интерпретируемость}: позволяла понять, как и почему принимается то или иное решение;
    \item  \textbf{Согласованность}: она должна обеспечивать согласованность результатов, т.е. предсказания ансамбля должны быть устойчивыми при малых изменениях в данных или в отдельных моделях.
\end{itemize}

\subsubsection*{Примеры агрегирующих функций:}
\begin{itemize}
    \item \textbf{Простое голосование} (\textit{simple voting}):
    $$F(b_1, \dots , b_T) = \frac{1}{T} \sum_{t=1}^T b_t$$
    \item \textbf{Взвешенное голосование} (\textit{weighted voting}):
    $$F(b_1, \dots, b_T) = \sum_{t=1}^T \alpha_t b_t, \quad \sum_{t=1}^T \alpha_t = 1, \quad \alpha_t \ge 0$$
    \item \textbf{Смесь алгоритмов} (\textit{mixture of experts}) с функциями компетентности (\textit{gating function}) $g_t: X \to \mathbb{R}$
    $$F(b_1, \dots, b_T, x) = \sum_{t=1}^T g_t(x) b_t(x)$$
\end{itemize}
\subsection{Проблема разнообразия базовых алгоритмов}
Явное преимущество композиции алгоритмов - возможность использовать независимые модели при построении ансамбля. Однако на практике  часто возникает ситуация, когда используется один и тот же алгоритм для создания ансамбля. Это, в свою очередь, приводит к тому, что модели в ансамбле не являются полностью независимыми.

Тем не менее, даже в случае использования одного и того же алгоритма, можно добиться некоторого разнообразия между моделями. Это может быть сделано за счет изменения параметров алгоритма, использования различных подмножеств данных для обучения (например, с помощью бэггинга, \textit{см. далее}) или применения различных техник предобработки данных. Кроме того, можно использовать различные способы инициализации или подходы к обучению, что также может привести к созданию моделей с различной производительностью.

\subsection{Методы стохастического ансамблирования} % Stochastic Ensemble Methods
Один из подходов к достижению этого разнообразия заключается в использовании методов рандомизации. Рандомизация позволяет генерировать различные обучающие подмножества данных, выбирать случайные подмножества признаков или изменять параметры моделей, что приводит к созданию разнообразного ансамбля и повышению его обобщающей способности. В данном разделе перечислены различные методы рандомизации, используемые для повышения разнообразия базовых моделей в ансамблях.

\ \

\textbf{Способы повышения разнообразия с помощью рандомизации:} 
\begin{itemize}
    \item \textit{bagging} (\textit{bootstrap aggregating}) – подвыборки обучающей выборки с возвращением: из исходной выборки размером \textit{N} образуется \textit{m} выборок, каждая из которых имеет тот же размер, что и исходный набор данных, но создаются они путем равномерного выбора элементов из исходного набора с возвратом. В каждую выборку попадает $1 - (1 - \frac{1}{\ell})^\ell$ уникальных объектов исходной выборки
    \item \textit{pasting} – случайные обучающие подвыборки 
    \item \textit{random subspaces} – случайные подмножества признаков 
    \item \textit{random patches} – случ. подмн-ва объектов и признаков 
    \item \textit{cross-validated committees} – выборка разбивается на $k$ блоков ($k$-fold) и делается $k$ обучений без одного блока 
\end{itemize}
Пусть $\mu: (G, U) \mapsto b$ – метод обучения по подвыборке $U \subseteq X^\ell$, использующий только признаки из $G \subseteq F^n = \{f_1, \dots, f_n\}$ 

\ \ 

\textbf{Алгоритм стохастического ансамблирования:}
\\
\textbf{Вход:} 

обучающая выборка $X^\ell$; параметры: 

$T$, $\ell'$ – объём обучающих подвыборок, 

$n'$ – размерность признаковых подпространств, 

$\varepsilon_1$ – порог качества базовых алгоритмов на обучении, 

$\varepsilon_2$ – порог качества базовых алгоритмов на контроле.
\\ 
\textbf{Выход:} 

базовые алгоритмы $b_t$,  $t = 1, \dots, T$: 
\begin{itemize}
    \item $U_t$ = случайная подвыборка объёма $\ell'$ из $X^\ell$; 
    \item $G_t$ = случайное подмножество мощности $n'$ из $F^n$; 
    \item $b_t = \mu(G_t, U_t)$; % $b_t = \mu(G_t, U_t)$;
    \item \textbf{если} $Q(b_t, U_t) > \varepsilon_1$ \textbf{то} не включать $b_t$ в ансамбль; 
    \item \textbf{если} $Q(b_t, X^\ell \setminus U_t) > \varepsilon_2$ \textbf{то} не включать $b_t$ в ансамбль; 
\end{itemize}
\ \
\textbf{Применение агрегирующей функции:} 

В простейшем случае используется простое голосование:
$$b(x) = \frac{1}{T} \sum_{t=1}^T  b_t(x)$$

\subsection*{Преобразование простого голосования во взвешенное}
Мы ожидаем, что некоторые базовые алгоритмы могут быть более точными и надежными, чем другие. Поэтому перейдем от простого голосования к взвешенному, где каждому классификатору присваивается определенный вес, отражающий его вклад в итоговое предсказание, для повышения точности и гибкости ансамбля.
\begin{itemize}
    \item \textbf{Линейная модель} над готовыми признаками $b_t(x)$:
    $$b(x) = \sum_{t=1}^T \alpha_t b_t(x)$$
    \item \textbf{Обучение:} МНК для регрессии, LR для классификации: 
    $$Q(\alpha, X^\ell) = \sum_{i=1}^\ell \mathcal{L}(b(x_i), y_i) \to \min_\alpha$$
    \textbf{Регуляризация:} $\alpha_t \ge 0$ либо LASSO: $\sum_{t=1}^T |\alpha_t| \
    \lambda$. 
\end{itemize}
Другой подход при выборе весов модели, заключается в предположении, что наши модели являются \textit{независимыми}. Тогда вохможно применение байесовского классификатора:
\begin{itemize}
    \item \textbf{Наивный байесовский классификатор} предполагает независимость с.в. $b_t(x)$ и даёт аналитическое решение: 
    $$\alpha_t = \ln \frac{1 - p_t}{p_t}, \quad t = 1, \dots, T,$$
    $p_t$ – оценка вероятности ошибки базового алгоритма $b_t$.
\end{itemize}




\subsection*{Задачи}
\subsubsection*{Задача 1}
Напишите декомпозицию алгоритма задачи классификации на K классов в общем случае. Каким в этом случае будет решающее правило С?
\subsubsection*{Задача 2}
Напишите декомпозицию алгоритма задачи регрессии в общем случае. Используется ли в данном случае решающее правило?
\subsubsection*{Задача 3}
Оцените уникальность подвыборки в методе bootstrap aggregating, для избыточного набора объектов исходной выборки, т.е. при $\ell \to \infty$.


\section{Нелинейные монотонные композиции}
\subsection{Основные обозначения} 
 \textbf{Определение 1}
 
 Произвольная линейная корректирующая операция $F(b_1,... , b_T ) = \alpha_1 b_1 + · · · + \alpha_T b_T$ с неотрицательными коэффициентами $\alpha_t$ является монотонным отображением из $R_T$ в Y и называется \textit{монотонной корректирующей операцией}.\\
 
 Искомый алгоритм $a$ имеет вид $a(x) = F(b_1(x), . . . , b_T (x))$
 
\subsection{Лемма о проведении монотонной функции через заданные точки.}
 
 \textbf{Определение 2}
 
 Пусть U, V -- произвольные частично уторядоченные множества. Совокупность пар $(u_i, v_i)_{i=1}^l$ из $U\times V$ назвыается \textit{монотонной}, если из $u_i\leq u_k$ следует $v_i\leq v_k$ для всех $j,k = 1,...,l.$\\
 
 \textbf{Лемма 1}
 
Пусть  $U, V$ -- произвольные частично упорядоченные множества. Монотонная функция $f : U \longrightarrow V$ такая, что $f(u_i) = v_i$ для всех $i = 1, . . . , l$, существует
тогда и только тогда, когда совокупность $(u_i
, v_i)^l_{i=1}$ монотонна.

\textbf{Доказательство.}

Необходимость вытекает из определения монотонной функции: если $f$ монотонна, то совокупность $(u_i,f(u_i))^l_{i=1}$ монотонна.

Докажем достаточность. Предполагая, что совокупность $(u_i,f(u_i))^l_{i=1}$ монотонна,
построим функцию $f$ в классе кусочно-постоянных функций. Определим для произвольного $u \in U $ множество индексов $I(u) = {k : u_i \leq u}$ и положим

\begin{equation*}
    f(u) =
    \begin{cases}
        \min\limits_{i=1,...,l}v_i, \text{ если } I(u) = \varnothing; \\
        \max\limits_{i\in I(u)}v_i, \text{ если } I(u) \in \varnothing .
    \end{cases}
\end{equation*}


Докажем, что функция f монотонна. Для произвольных $u$ и $u\prime $ из $u_i \leq u$ следует $I(u)\subseteq I( u\prime)$, значит $f(u)\leq f(u\prime )$.
Докажем, что $f(u_i) = v_i$ для всех $i = 1, . . . , l$. Множество $I(u_i)$ не пусто, так
как $i \in I(u_i)$. Для любого $k \in I(u_i)$ в силу монотонности $(u_i, v_i)^l_{i=1}$ из $u_k \leq u_i$ следует $v_k \leq v_i$. Но тогда $max_{k\in I(u_i)} v_k$ достигается при $k = i$, откуда следует $f(u_i) = v_i$.$\blacksquare$\\




Предложенный способ построения функции $f$ мало пригоден для практических нужд.
 В регрессионных задачах желательно, чтобы функция $f$ была гладкой или хотя бы непрерывной,здесь  $f$ кусочно-постоянна. В задачах классификации  -- чтобы разделяющая поверхность проходила как можно дальше от точек выборки, здесь разделяющая полоса целиком относится к классу 0.


\subsection{Оптимизация базовых алгоритмов}

Обозначим через $u_i$ вектор значений базовых алгоритмов на объекте $x_i$, через $f_i$ — значение, выданное алгоритмом a(x) на объекте $x_i$:
\[u_i = (b_1(x_i), . . . , b_t(x_i));\]
\[f_i = a(x_i) = F(b_1(x_i), . . . , b_t(x_i)) = F(u_i); \; i = 1, . . . , l.\]
В новых обозначениях условие корректности алгоритма $a(x_i) = y_i$ примет вид
\[F(u_i) = y_i
, \; i = 1, . . . , l. 
\]\\

\textbf{Определение 3} 

Пусть $V$ — произвольное упорядоченное множество. Пара индексов $(j, k)$
называется дефектной для функции $b : X \rightarrow V$ , если $y_j < y_k$ и $b(x_j ) > b(x_k)$. Дефектом функции $b(x)$ называется множество всех её дефектных пар:
\[D(b) = {(j, k): y_j < y_k \wedge b(x_j ) > b(x_k)} .\]


Следовательно любой монотонный оператор, а следовательно и алгоритм $a(x) = F(b1(x), . . . , bt(x)) $ допустят ошибку в точке $x_j$ или $x_k$ при дефектной паре $(j, k)$.
\\

\textbf{Определение 4} 

Совокупным дефектом операторов $b_1, . . . , b_t$ называется множество
$D_T (b_1, . . . , b_t) = D(b_1) \cap ... \cap D(b_t) = {(j, k): y_j < y_k \wedge u_j > u_k}.$
Будем также пользоваться сокращённым обозначением $D_t = D_t(b_1, . . . , b_t).$
\\
Для любой пары $(j, k)$ из совокупного дефекта и любой монотонной функции $F$ алгоритм $a(x) = F(b_1(x), . . . , b_t(x)) $ допустит ошибку хотя бы на одном из двух объектов $x_j$ или $x_k$.\\

\textbf{
Теорема 1} 

 Монотонная функция $F : R^t \rightarrow Y $, удовлетворяющая условию корректности существует тогда и только тогда, когда совокупный дефект операторов $b_1, . . . , b_t$ пуст. При этом дефект алгоритма $a(x) = F(b_1(x), . . . , b_t(x)) $ также пуст.


\textbf{Доказательство.}

Справедлива следующая цепочка равносильных утверждений:
а) совокупный дефект пуст: $D_t(b_1, . . . , b_t) = \varnothing$;
б) для любых $j, k$ не выполняется $(u_k \leq u_j ) \wedge (y_j < y_k)$ (согласно Опр. 1.6);
в) для любых $j, k$ справедлива импликация $(u_k \leq u_j ) \rightarrow (y_k \leq y_j )$;
г) совокупность пар $(u_i, y_i)^l_{i=1}$ монотонна (согласно Опр. 1.4);
д) существует монотонная функция $F : R_t \rightarrow Y$ такая, что $F(u_i) = y_i$ для всех
$i = 1, . . . , l$ (согласно Лемме 1.5).
Из утверждения д) следует, что условия $y_j < y_k$ и $F(u_j ) > F(u_k)$ не могут
выполняться одновременно, значит, дефект $D(F(b_1, . . . , b_t))$ также пуст. $\blacksquare$\\


Таким образом, для корректной работы алгоритма необходимо построить операторы $ b_1, . . . , b_t$
, совокупный дефект которых пуст. А добавление такого оператора $b$, что 
\begin{equation}\label{iter}
b_t (x_j) < b_t(x_k) (j, k) \in D_{t-1}
\end{equation}
приводит к устранению дефектной пары.\\

\textbf{Теорема 2 (о сходимости)}. 

Пусть на первом шаге построен оператор $b_1$, и семейство операторов $B$ выбрано так, что для любой подвыборки $X^{2m}$
длины $2m, m > 1$, найдётся оператор $b\in B$ и монотонная корректирующая операция $F$, удовлетворяющие системе ограничений
\[
F(b(x_i)) = y_i
, x_i \in X^{2m}.\]
Тогда итерационный процесс, аналогичный последовательному построению смеси алгоритмов, приводит к построению корректной композиции
$a = F(b_1, . . . , b_T )$ за конечное число шагов $T \leq [D(b_1)/m]+ 1$.


\textbf{Доказательство.}

Рассмотрим t-й шаг, t > 2, итерационного процесса . Если $\vert D_{t-1}\vert > m$,
то выберем некоторое m-элементное подмножество совокупного дефекта $\Delta_{t-1} \subseteq D_{t-1}$.
Если $\vert D_{t-1}\vert \leq m$, то положим $\Delta{t-1} = D_{t-1}$. Рассмотрим подмножество объектов
выборки, образующих всевозможные дефектные пары из $\Delta{t-1}$:
\[ U = \lbrace x_i \in X^l \exists k : (k, i) \in \Delta_{t-1} \; \text{или} \; (i, k) \in \Delta_{t-1} \rbrace\].

Очевидно, мощность $U$ не превышает 2m. Согласно условию теоремы существует оператор $b_t \in B$ и монотонная корректирующая операция $F$, удовлетворяющие системе
ограничений $F(b_t(x_i)) = y_i$ при всех $x_i \in U$. Но тогда для оператора $b_t$ выполняется
также система ограничений
\[ b_t(x_j) < b_t(x_k) \; \, (j, k) \in \Delta_{t-1}\]


Докажем это от противного: пусть $b_t(x_k) \leq b_t(x_j )$, тогда $F(b_t(x_k)) \leq F(b_t(x_j ))$, следовательно, $y_k \leq y_j$
, что противоречит условию $y_j < y_k$, входящему в определение
дефектной пары $(j, k)$.
Если выбирать операторы $b_t , \; t = 2, 3, . . .$ указанным способом, то мощность
совокупного дефекта будет уменьшаться, как минимум, на m на каждом шаге итерационного процесса: $\vert D_t \vert \leq \vert D_{t-1}\vert -m$. Для полного устранения дефекта потребуется не более $\lceil D(b_1)/m\rceil$
операторов, не считая $b_1$. $\blacksquare$
\\

Процесс последовательного построения базовых алгоритмов организуется
по принципу \textit{наискорейшего устранения дефекта}.
Если специальным образом задать веса
обучающих объектов, мощность совокупного дефекта можно оценить сверху
обычным функционалом числа ошибок.

Следующая теорема показывает, что в случае классификации вес i-го объекта
можно положить равным числу пар из совокупного дефекта $D_{t-1}$, в которых участвует данный объект.\\

\textbf{Теорема 3} 

Если $Y = {0, 1}$ и функция потерь имеет вид $\overset{\sim}{L}(b, y) =[[b > 0]\neq y]$ , то справедлива оценка \[\vert D_t(b_1, . . . , b_t)\vert \leq \operatornamewithlimits{\sum}^l_{i=1} w_i \overset{\sim}{L} (b_t(x_i), y_i); \]
\[w_i = \vert D(i)\vert ;\]
\[D(i) = \lbrace x_k \in X^l \vert (k, i) \in D_{t-1} \text{ или } (i, k) \in D_{t-1} \rbrace; \; i = 1, . . . , l.\]

\textbf{Доказательство.}

Обозначим через $\beta_i = b_t(x_i)$ значение t-го оператора на i-м обучающем объекте, через $L_i = \overset{\sim}{L}(\beta_i, y_i) = [\beta_i > 0] \neq y_i$

— значение функции потерь, равное 1, если
оператор $b_t$ допускает ошибку на i-м объекте, 0 в противном случае.
Для любых действительных $\beta_j, \beta_k$ справедливо неравенство
\[[\beta_j > \beta_k] \leq [\beta_j > 0] + [\beta_k \leq 0]\].



Если $y_j < y_k$, то из двухэлементности множества Y следует:
\[y_j = 0; \; [\beta_j > 0] =[\beta_j > 0] \neq 0 = [\beta_j > 0] \neq y_j = L_j;\]
\[y_k = 1; \; [\beta_k \leq 0] =
[\beta_k > 0] \neq 1=[\beta_k > 0] \neq y_k= L_k.\]

Используя представление $D_t = D_{t-1} \bigcap D(b_t)$, распишем мощность совокупного
дефекта в виде суммы по парам объектов и применим оценку $[\beta_j > \beta_k] \in L_j + L_k$:
\[\vert D_t \vert = \operatornamewithlimits{\sum}_{(j,k)\in D_{t-1}} [(j, k) \in D(b_t)]=
\operatornamewithlimits{\sum}_{(j,k)\in D_{t-1}}  [\beta_j > \beta_k] \leq 
\operatornamewithlimits{\sum}_{(j,k)\in D_{t-1}}(L_j + L_k) =\]
\[ = \operatornamewithlimits{\sum}^l_{\operatornamewithlimits{j=1}\limits_{y_j=0}} L_j
\operatornamewithlimits{\sum}^l_{k=1}[(j, k) \in D_{t-1}] +
\operatornamewithlimits{\sum}^l_{\operatornamewithlimits{k=1}\limits_{y_k=1}} L_k
\operatornamewithlimits{\sum}^l_{j=1}[(j, k) \in D_{t-1}]=\]
\[=\operatornamewithlimits{\sum}^l_{i\in 1} L_i \operatornamewithlimits{\sum}^l_{k=1} [(k, i) \in D_{t-1}\text{ или }(i, k)\in D_{t-1}=\operatornamewithlimits{\sum}^l_{i\in 1} L_i \vert D(i)\vert.\] $\blacksquare$\\





%\begin{figure}[h!]
%	\centering
%	\includegraphics[height=0.3\textheight]{signal}
%	\caption{Зависимость тока от времени для GaAs в диоде Ганна}
%\end{figure}


\subsection*{Задачи}
\textbf{\emph{Задача 1}}

Имеется алгоритм $a(x)=F(b_1(x)...,b_t(x))$, где $F$ монотонная корректирующая операция. 
Если $\forall i; \; i = 1,...,t; \; b_i\in B$, функция $a \in A$, является ли верным утверждение, что $A=B$?

\textit{Решение}

Класс $B$ может накладывать определенные ограничения на функции, которые он содержит (например, линейность, непрерывность, ограниченность). Монотонная операция $F$ может изменить эти ограничения.

 Монотонная операция F может расширить или сузить множество функций, которые могут быть получены. Eсли $F$ – нелинейная функция (например, $F(x_1, ..., x_t) = max(x_1, ..., x_t))$, то класс $A$ будет содержать нелинейные функции (даже если все $b_i$ линейны), которые не принадлежат классу $B$. В этом случае $A \neq B$ а именно $A \supset B $.\\

\textbf{\emph{Задача 2}}

Напишите пример монотонной корректирующей операции $F$, такой что в предыдущей задаче достигнется развенство классов $A$ и $B$.


\textit{Решение}

Пусть класс $B$ содержит только линейные функции вида $b_i(x) = k_i x + c_i$. Если $F(x_1, x_2) = x_1 + x_2$, то класс $A$ также будет содержать только линейные функции. В этом случае $A = B$.\\

\textbf{\emph{Задача 3}}

Рассмотрим класс функций $F$, представляющих собой композиции монотонно неубывающих функций. Пусть $b_i: \mathbb{R} \rightarrow \mathbb{R}$ для $i = 1, ..., t$ — монотонно неубывающие функции, и $g_i: \mathbb{R}^n \rightarrow \mathbb{R}$ для $i = 1, ..., t$ — также монотонно неубывающая функция. Тогда функция $F(x_1, ..., x_n) = g(f_1(x_1), ..., f_n(x_n))$ принадлежит классу $F$.

Верно ли следуюшее утверждение? Если функция $h(x_1, x_2) \in F$ удовлетворяет условиям:

\begin{itemize}
\item $h(0, 0) = 0$
\item $h(1, 1) = 1$
\item $h(x, y) = h(y, x)$
\item Для любых $x_1, x_2 \geq 0$ выполняется $h(x_1, x_2) \geq max(x_1, x_2)$,

\end{itemize}


то функция $h(x, y)$ тождественно равна $max(x, y)$ при $x, y \in [0, 1]$.\\

\textit{Решение}

Докажем от противного.

Пусть $\exists h(x_1, x_2) \in F, \; h(x_1, x_2) \neq max(x_1, x_2)$ на [0, 1], удовлетворяющая условиям 1-4.  Значит существует хотя бы одна пара $(x_0, y_0) \in [0, 1] \; : \; h(x_0, y_0)> max(x_0, y_0)$.

Предположим $x_0 \geq y_0$. Тогда $h(x_0, y_0) > x_0$.

Так как $h \in F$,  она является композицией монотонно неубывающих функций.  Это означает, что если $x_1 \geq x_2$ и $y_1 \geq y_2$, то $h(x_1, y_1) \geq h(x_2, y_2)$.

Рассмотрим точку $(x_0, x_0)$.  По свойству 3,$ h(x_0, y_0) = h(y_0, x_0)$.  По условию 4, $h(x_0, x_0) \geq x_0$.  Но так как $h(x_0, y_0) > x_0$ и $y_0 \neq x_0$,  из монотонности $h$ следует   $h(x_0, x_0) \geq h(x_0, y_0) > x_0$.

Теперь рассмотрим случай, когда $x_0 = 1$ и $y_0 < 1$.  Тогда $h(1, y_0) > 1$  (поскольку $h(1,y_0) > max(1, y_0) = 1$). Но это противоречит условию 4, что $h(x,y) \neq 1 \; \forall x, y \in [0, 1]$ ввиду монотонного неубывания $h(x,y)$ по обоим аргументам и $h(1,1) = 1$.  Если бы $h(1, y_0) > 1$, то и $h(1,1)$ должна быть больше 1, что противоречит условию.

Аналогично, если  $x_0 < 1$ и $y_0 = 1$.

Если же $0 < x_0 < 1$ и $0 < y_0 < 1$, и  $h(x_0, y_0) > max(x_0, y_0)$, то можно построить последовательность точек, приближающихся к (1, 1),  где значение $h$ будет продолжать расти, что приведет к противоречию с условием $h(1, 1) = 1$ из-за монотонности $h$.

Следовательно изначальное предположение неверно и $h(x,y)\equiv max(x,y).\blacksquare$

